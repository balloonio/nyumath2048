{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 3: Linear Algebra II\n",
    "\n",
    "Topics:\n",
    "\n",
    "* Least Square\n",
    "* Eigenvalue and Eigenvector \n",
    "* QR Decomposition\n",
    "* Principal Component Analysis\n",
    "* Singular Value Decomposition\n",
    "\n",
    "$\\renewcommand{bt}[1]{\\bs{\\tilde #1}} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# initialize the environment\n",
    "%pylab inline\n",
    "import sympy as sp\n",
    "import fmt\n",
    "import pandas as pd\n",
    "\n",
    "sp.init_printing(use_latex = True)\n",
    "from IPython.display import display\n",
    "lecture = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Least Square\n",
    "\n",
    "Robert Wilson: Consciousness itself is an infinite regress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Least square problem\n",
    "\n",
    "Given an unknown scalar function $\\renewcommand{bs}{\\boldsymbol} y = y(\\boldsymbol x)$, where the argument $\\bs x$ is a $n$ dimensional vector.\n",
    "\n",
    "* We have $m > n$ observations: $X$ represents inputs and $\\bs y$ represents outputs:\n",
    "\n",
    " $$\\small X = \\pmatrix{\\bs x_1^T \\\\ \\bs x_2^T \\\\ \\vdots \\\\ \\bs x_m^T}, \\;\\; \\bs y = \\pmatrix{y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m}$$\n",
    " \n",
    "* We want to find a factor loading vector $\\bs \\beta$ so that the linear function $\\hat y = \\bs x^T \\bs \\beta$ best approximate the observed inputs/outputs.\n",
    "* $X \\bs \\beta = \\bs y$ is over-determined, no vector $\\bs \\beta$ solves it exactly\n",
    "* Instead, we find the $\\bs \\beta^* $ that minimizes $\\Vert X \\bs{\\beta - y}\\Vert_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pseudo inverse:\n",
    "\n",
    "$$ \\bs \\beta^* = \\text{argmin}_{\\bs {\\beta}} \\Vert X \\bs{\\beta - y} \\Vert_2$$\n",
    "Which can be easily solved using Matrix calculus:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\Vert X \\bs{\\beta - y} \\Vert_2^2 &=& (X \\bs{\\beta - y})^T (X \\bs{\\beta - y}) \\\\\n",
    "&=& \\bs{\\beta}^T X^T X \\bs{\\beta} - 2 \\bs{\\beta}^TX^T\\bs y + \\bs y^T\\bs y\\\\\n",
    "\\frac{\\partial}{\\partial \\bs \\beta^T} \\Vert X \\bs{\\beta - y} \\Vert_2^2 &=& 2 X^TX \\bs \\beta  - 2X^T \\bs y  = \\bs 0 \\\\\n",
    "\\bs \\beta^* &=& (X^TX)^{-1}X^T\\bs y\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "* The solution is equivalent to linear regression, but the later requires stronger assumptions for forecasting.\n",
    "* $X^+ = (X^TX)^{-1}X^T$ is also known as the psuedo inverse of $X$ because $X^+X = I$\n",
    "  * note $XX^+$ is not even a valid matrix multiplication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Condition number of non-square matrix\n",
    "\n",
    "We already proved that an invertible matrix $A$'s condition number is \n",
    "\n",
    "$$k(A) = \\Vert A \\Vert \\Vert A^{-1} \\Vert$$\n",
    "\n",
    "We can extend this to non-square matrix using pseudo-inverse, following the same proof.\n",
    "\n",
    "$$k(A) = \\Vert A \\Vert \\Vert A^{+} \\Vert$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ridge regression\n",
    "\n",
    "The $\\bs \\beta$ that solves the least square problem may have large magnitude:\n",
    "* often cause problems in practice\n",
    "\n",
    "Ridge regression adds a magnitude penalty to the objective function of least square: \n",
    "\n",
    "$$\\begin{eqnarray} l(\\bs \\beta) &=& \\Vert X \\bs{\\beta - y} \\Vert_2^2 + \\lambda \\Vert W \\bs \\beta \\Vert_2^2 \\\\ &=&  \\bs{\\beta}^T X^T X \\bs{\\beta} - 2 \\bs{\\beta}^TX^T\\bs y + \\bs y^T \\bs y + \\lambda \\bs \\beta^T W^TW \\bs \\beta \\\\\n",
    "\\frac{\\partial l}{\\partial \\bs \\beta^T} &=& 2X^TX \\bs \\beta   - 2X^T \\bs y + 2 \\lambda W^T W \\bs \\beta = \\bs 0 \\\\\n",
    "\\bs \\beta &=& (X^TX + \\lambda W^TW)^{-1}X^T\\bs y\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "* $W$ is a diagonal weighting matrix for the elements in $\\bs \\beta$, e.g. $W = I$ is often a good start\n",
    "* $\\lambda$ is a scalar penalty rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ridge regression example\n",
    "\n",
    "We draw many pairs of $x, y, z$ from the following linear model: \n",
    "\n",
    "$$  y = 2x + 0.1z + 5 + \\epsilon $$\n",
    "\n",
    "* $x, z$ are standard normal, $z$ represents an insignificant feature (or accidental correlation)\n",
    "* $\\epsilon$ is a standard normal noise\n",
    "\n",
    "We regress the vector $\\bs y$ against $X = [\\bs x, \\bs x + .0001 \\bs z, \\bs 1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Fmt' object has no attribute 'displayDF'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ff794e40d5bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                   columns=['$\\\\bs x$', '$\\\\bs x+.0001\\\\bs z$', '$\\\\bs 1$'])\n\u001b[1;32m     20\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'$\\Vert X\\\\bs \\\\beta - \\\\bs y \\\\Vert_2$'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me_ols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_rr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mfmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"4g\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Fmt' object has no attribute 'displayDF'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "n = 5000\n",
    "x = np.random.normal(size=n)\n",
    "z = np.random.normal(size=n)\n",
    "y = 2*x + 5 + 0.1*z + np.random.normal(size=n)\n",
    "\n",
    "xs = np.array([x, x + 0.0001*z, np.ones(len(x))]).T\n",
    "\n",
    "q = np.eye(len(xs.T))\n",
    "lbd = .1\n",
    "b_ols = np.linalg.inv(xs.T.dot(xs)).dot(xs.T).dot(y)\n",
    "e_ols = np.linalg.norm(y - xs.dot(b_ols), 2)\n",
    "\n",
    "b_rr = np.linalg.inv(xs.T.dot(xs) + lbd*q.T.dot(q)).dot(xs.T).dot(y)\n",
    "e_rr = np.linalg.norm(y - xs.dot(b_rr), 2)\n",
    "\n",
    "df = pd.DataFrame(np.array([b_ols, b_rr]), index=['Least square', 'Ridge regression $\\\\lambda=%2g$' % lbd], \n",
    "                  columns=['$\\\\bs x$', '$\\\\bs x+.0001\\\\bs z$', '$\\\\bs 1$'])\n",
    "df['$\\Vert X\\\\bs \\\\beta - \\\\bs y \\\\Vert_2$'] = [e_ols, e_rr]\n",
    "fmt.displayDF(df, \"4g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ridge regression is more robust, it works even if $X$ is not fully ranked (colinearity)\n",
    "* often used for constructing hedging portfolios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regularization\n",
    "\n",
    "Ridge regression is an example of regularization. Two other popular regularizations are:\n",
    "\n",
    "* LASSO regression: resulting in sparse $\\bs \\beta$\n",
    "\n",
    "$$\n",
    "l(\\bs \\beta) = \\Vert X \\bs{\\beta - y} \\Vert_2^2 + \\lambda \\Vert W \\bs \\beta \\Vert_1\n",
    "$$\n",
    "\n",
    "* Elasticnet: in between LASSO and Ridge\n",
    "\n",
    "$$\n",
    "l(\\bs \\beta) = \\Vert X \\bs{\\beta - y} \\Vert_2^2 + \\lambda_1 \\Vert W \\bs \\beta \\Vert_1 + \\lambda_2 \\Vert W \\bs \\beta \\Vert_2^2\n",
    "$$\n",
    "\n",
    "Both LASSO and Elasticnet are equivalent to support vector machine (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Eigenvalues and Eigenvectors\n",
    "\n",
    "eigen-: characteristic, origin: German"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Definitions\n",
    "\n",
    "For a square matrix $A$ of size $n \\times n$, if there exists a vector $\\bs {v \\ne 0}$ and a scalar $\\lambda \\ne 0$ so that:\n",
    "\n",
    "$$ A \\bs v = \\lambda \\bs v $$\n",
    "\n",
    "* $\\bs v$ is an eigenvector,  $\\lambda$ is the corresponding eigenvalue\n",
    "* $A$ only changes $\\bs v$'s magnitude, but not direction\n",
    "* $\\lambda$ can be complex even for real $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Eigen vector/value conventions\n",
    "\n",
    "\n",
    "$$ A \\bs v = \\lambda \\bs v $$\n",
    "\n",
    "Eigenvectors are usually specified as unit L2 vectors of $\\Vert \\bs v \\Vert_2^2 = \\bs v^T \\bs v = 1$. \n",
    "  * $\\bs v$ and $- \\bs v$ are equivalent unit eigenvectors\n",
    "  \n",
    "Eigenvalues are named in descending order: $\\lambda_1 \\ge \\lambda_2 \\ge ... \\ge \\lambda_n $ \n",
    "* their corresponding eigen vectors are $\\bs {v_1, v_2, ..., v_n}$ \n",
    " \n",
    "Can eigenvalues be negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Characteristic equation\n",
    "Rewrite the equation as:\n",
    "\n",
    "$$ (A - \\lambda I) \\bs {v = 0}$$\n",
    "\n",
    "It has non-zero solution if and only if:\n",
    "\n",
    "$$ \\det(A - \\lambda I) = 0$$ \n",
    "\n",
    "a polynomial equation of degree $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Properties of eigenvalues\n",
    "\n",
    "For matrix $A$ of size $n \\times n$:\n",
    "* there can be $n$ distinct eigenvalues at most\n",
    "* eigenvalues can be negative or complex\n",
    "* $\\prod_{i}\\lambda_i = \\det( A )$\n",
    "* $\\sum_i\\lambda_i = \\text{tr}(A)$\n",
    "\n",
    "It is difficult to solve the characteristic function for $\\lambda_i$, especially for large $n$.\n",
    "* But once we have eigenvalues, it is easy to find corresponding eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Independence of eigenvectors\n",
    "\n",
    "Eigen vectors of distinct eigen values are linearly independent.\n",
    "\n",
    "* proof: suppose the first k-1 eigen vectors are independent, but not the k-th:\n",
    "  \n",
    "$$\\begin{array} \\\\\n",
    "   \\bs{v_k} &= \\sum_{i=1}^{k-1}c_i\\bs{v_i} \\\\\n",
    "   \\lambda_k \\bs{v_k} &= \\sum_{i=1}^{k-1}c_i\\lambda_i\\bs{v_i} =  \\lambda_k \\sum_{i=1}^{k-1}c_i\\bs{v_i} \\\\\n",
    "\\bs 0  &= \\sum_{i=1}^{k-1}c_i(\\lambda_i - \\lambda_k) \\bs{v_i} \n",
    "   \\end{array}\n",
    "   $$\n",
    "   \n",
    "   this leads to contradiction.\n",
    "* If $A$ has $n$ distinct eigenvalues, then all the eigen vectors form a basis for the vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Properties of eigenvectors\n",
    "\n",
    "If $A$ has $n$ distinct eigen values, we can write:\n",
    "\n",
    "$$ A R = R \\Lambda$$\n",
    "\n",
    "* where each column of $R$ is a eigenvector, and $\\Lambda$ is a diagonal matrix of eigenvalues\n",
    "\n",
    "\n",
    "$R$ is invertible because eigen vectors are all independent:\n",
    "\n",
    "$$\\begin{array} \\\\\n",
    "R^{-1} A &= \\Lambda R^{-1} \\\\\n",
    " A^*(R^{-1})^* &= (R^{-1})^* \\Lambda^* \\\\\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Eigenvalue decomposition\n",
    "\n",
    "\n",
    "$$A R = R\\Lambda \\iff A^*(R^{-1})^* = (R^{-1})^* \\Lambda^* $$\n",
    "\n",
    "If $A$ is real and symmetric: $A^* = A$:\n",
    "\n",
    "* $\\Lambda^* = \\Lambda$: all eigenvalues are real\n",
    "* we can consider real eigen vectors only without losing generality\n",
    "* $R$ is orthogonal: $(R^{-1})^* = (R^{-1})^T = R \\iff RR^T = I$\n",
    "\n",
    "$A =  R\\Lambda R^T$, this diagonalization is called the eigenvalue decomposition (EVD)\n",
    "* all eigenvalues are positive if and only if $A$ is SPD\n",
    "* applies even when there are duplicated eigen values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Eigenvectors and maximization\n",
    "\n",
    "If $A$ is real and symmetric: \n",
    "\n",
    "* $\\bs v_1$ maximizes the $\\bs u^T A \\bs u$ among all L-2 unit vectors, i.e., $\\bs u^T \\bs u = 1$\n",
    "\n",
    "Apply Lagrange multiplier:\n",
    "\n",
    "$$\\begin{array}\n",
    "\\\\ l &= \\bs u^T A \\bs u - \\gamma (\\bs u^T \\bs u - 1) \\\\\n",
    "\\frac{\\partial l}{\\partial \\bs u^T} &= 2 A \\bs u - 2\\gamma \\bs u = \\bs 0 \\iff A \\bs u = \\gamma \\bs u\n",
    "\\end{array}$$\n",
    "\n",
    "* the solution must be an eigenvector \n",
    "* since $\\bs v_i^T A \\bs v_i = \\lambda_i$, $\\bs v_1$ is the solution because $\\lambda_1$ is the largest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This process can be repeated to find all eigenvalues and vectors:\n",
    "\n",
    "* $\\bs v_2$ maximizes $\\bs u^T A \\bs u$ for all unit $\\bs u$ that is orthogonal to $\\bs v_1$, i.e, $\\bs u^T \\bs v_1 = 0$.\n",
    "* $\\bs v_i$ maximizes amongst those unit $\\bs u$ that are orthogonal to $\\bs v_1, ..., \\bs v_{i-1}$\n",
    "* In the case of duplicated eigen values, $\\bs v_i$ is not unique, we can pick any $\\bs v_i$ and continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Matrix similarity\n",
    "\n",
    "Square matrix $B$ and $A$ are similar if $B = PAP^{-1}$: \n",
    "* similar matrix have identical eigenvalues:\n",
    "\n",
    "$$ AR = P^{-1}BP R = R \\Lambda \\iff B (PR) = (PR) \\Lambda $$ \n",
    "\n",
    "* $B$ is also called a similarity transformation of $A$, representing the same operation in different basis\n",
    "* $P$ is orthogonal if $A$ is real and symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Solving eigenvalues\n",
    "\n",
    "It is very challenging to solve eigen values for large matrices.\n",
    "* Solving characteristic equation is not numerically feasible.\n",
    "* QR algorithm is an important breakthrough in numerical analysis.\n",
    "\n",
    "The basic idea of solving eigen values of matrix $A$:\n",
    "* Find a similarity transformation $B = PAP^{-1}$ so that $B$ is a triangular matrix\n",
    "* $A$ and $B$ has identical eigenvalues\n",
    "* Eigen values of the triangular matrix $B$ are simply its diagonal elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# QR Decomposition\n",
    "\n",
    "Nassim Taleb: Decomposition, for most, starts when they leave the free, social, and uncorrupted college life for the solitary confinement of professions and nuclear families."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## QR decomposition\n",
    "\n",
    "Any real matrix $A$ can be decomposed into $A = QR$:\n",
    "* $Q$ is orthogonal ($QQ^T = I$)\n",
    "* $R$ is upper triangular, with the same dimension as $A$\n",
    "\n",
    "QR decomposition is numerically stable\n",
    "* making EVD analysis of large matrices a routine practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\overbrace{\\pmatrix{\n",
    "* & * & * & * \\\\  \n",
    "* & * & * & * \\\\  \n",
    "* & * & * & * \\\\  \n",
    "* & * & * & * \\\\  \n",
    "* & * & * & * \\\\  \n",
    "* & * & * & * \n",
    "}}^A = \n",
    "\\overbrace{\\pmatrix{\n",
    "* & * & * & * & * & * \\\\  \n",
    "* & * & * & * & * & * \\\\  \n",
    "* & * & * & * & * & * \\\\  \n",
    "* & * & * & * & * & * \\\\  \n",
    "* & * & * & * & * & * \\\\\n",
    "* & * & * & * & * & *\n",
    "}}^Q \n",
    "\\;\\;\n",
    "\\overbrace{\\pmatrix{\n",
    "* & * & * & *\\\\ \n",
    "0 & * & * & *\\\\ \n",
    "0 & 0 & * & * \\\\\n",
    "0 & 0 & 0 & * \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "}}^R$$\n",
    "\n",
    "\n",
    "If $A$ is not fully ranked:\n",
    "* $Q$ remains full rank (as all orthogonal matrices)\n",
    "* $R$ have more 0 rows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## QR algorithm for solving eigenvalues\n",
    "\n",
    "\n",
    "QR algorithm is one of the most important numerical methods of the 20th century <a href=https://www.siam.org/pdf/news/637.pdf>link</a>\n",
    "\n",
    "Start with $A_0 = A$, then iterate:\n",
    "\n",
    "* run a QR decomposition of $A_k$: $A_k = Q_kR_k$\n",
    "* set $A_{k+1} = R_kQ_k = Q_k^{-1}A_kQ_k$, $A_{k+1}$ therefore has the same eigen values as $A_k$\n",
    "* stop if $A_k$ is adequately upper triangular\n",
    "* the eigven values are the diagonal elements of $A_k$\n",
    "* guaranteed to converge if $A$ is real and symmetric\n",
    "\n",
    "This algorithm is unconditionally stable because only orthogonal transformations are used.\n",
    "\n",
    "* $A$ is often transformed using a similarity transformation to a near upper triangle before applying the QR algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example of QR algorithm\n",
    "\n",
    "Find the eigenvalues of the following matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def qr_next(a) :\n",
    "    q, r = np.linalg.qr(a)\n",
    "    return r.dot(q)\n",
    "\n",
    "a = np.array([[5, 4, -3, 2], [4, 4, 2, -1], [-3, 2, 3, 0], [2, -1, 0, -2]])\n",
    "A = sp.MatrixSymbol('A', 4, 4)\n",
    "A1 = sp.MatrixSymbol('A_1', 4, 4)\n",
    "Q = sp.MatrixSymbol('Q_0', 4, 4)\n",
    "R = sp.MatrixSymbol('R_0', 4, 4)\n",
    "\n",
    "fmt.displayMath(fmt.joinMath('=', A, sp.Matrix(a)), pre=\"\\\\tiny \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QR decomposition of $A_0 = A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "q, r = np.linalg.qr(a)\n",
    "fmt.displayMath(fmt.joinMath('=', Q, sp.Matrix(q).evalf(3)), fmt.joinMath('=', R, sp.Matrix(r).evalf(3)), pre=\"\\\\tiny \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$A$ at the start of the next iteration, and after 20 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ii = 20\n",
    "d = np.copy(a)\n",
    "for i in range(ii) :\n",
    "    d = qr_next(d)\n",
    "    \n",
    "An = sp.MatrixSymbol('A_%d' % ii, 4, 4)\n",
    "fmt.displayMath(fmt.joinMath('=', A1, sp.Matrix(r.dot(q)).evalf(3)), fmt.joinMath('=', An, sp.Matrix(np.round(d, 3))), pre=\"\\\\tiny \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Householder transformation\n",
    "\n",
    "An orthogonal transformation representing reflection over a hyper plane, with a normal vector $\\hat {\\bs u}$ with unit L-2 norm:\n",
    "\n",
    "$$ H \\bs x = (I - {2\\bs{\\hat u \\hat u}^T}) \\bs x = \\bs x - 2 \\bs{\\hat u} (\\hat{\\bs u}^T \\bs x)  $$\n",
    "\n",
    "Household transformation can reflect an arbitrary vector $\\bs x$ to $ \\Vert \\bs x \\Vert_2 \\hat{\\bs e}_1$, where $\\hat{\\bs e}_1$ can be any unit vector. \n",
    "\n",
    "<center><img src = \"img/householder.png\" height=300 width=300></center>\n",
    "\n",
    "* It is obvious that: $\\bs{\\hat u}$ must be in the direction of $\\bs u = \\frac{1}{2}\\left(\\bs x - \\Vert \\bs x \\Vert_2 \\hat{\\bs e}_1\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## QR decomposition using Householder\n",
    "\n",
    "We show how to use Householder transformation to perform QR decomposition of the $A$ in previous example.\n",
    "\n",
    "* The first step, zero out the lower triangle of the first column by a Householder transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def householder(x0, e=0) :\n",
    "    n = len(x0)\n",
    "    e1 = np.zeros(n-e)\n",
    "    x = x0[e:]\n",
    "    e1[0] = np.linalg.norm(x, 2)\n",
    "    u = x - e1\n",
    "    v = np.matrix(u/np.linalg.norm(u, 2))\n",
    "    hs = np.eye(n-e) - 2*v.T*v\n",
    "    h = np.eye(n)\n",
    "    h[e:,e:] = hs\n",
    "    return h\n",
    "\n",
    "x, u, e1, Q, R = sp.symbols(\"x, u, e_1, Q, R\")\n",
    "xn = sp.symbols(\"\\Vert{x}\\Vert_2\")\n",
    "b = a[:, 0]\n",
    "c = np.zeros(len(b))\n",
    "c[0] = np.linalg.norm(b, 2)\n",
    "\n",
    "fmt.displayMath(fmt.joinMath('=', A, sp.Matrix(np.round(a))), fmt.joinMath('=', x, sp.Matrix(a[:,0])), \n",
    "                fmt.joinMath('=', xn, np.round(np.linalg.norm(b, 2), 3)),\n",
    "                fmt.joinMath('=', e1, sp.Matrix([1, 0, 0, 0])),\n",
    "                fmt.joinMath('=', u, sp.Matrix(a[:,0]-c).evalf(3)), pre=\"\\\\tiny \")\n",
    "\n",
    "fmt.displayMath(\"\\;\")\n",
    "    \n",
    "h1 = householder(a[:, 0], 0)\n",
    "H1 = sp.MatrixSymbol('H_1', 4, 4)\n",
    "a1 = h1.dot(a)\n",
    "fmt.displayMath(fmt.joinMath('=', H1, sp.Matrix(np.round(h1, 3))), \n",
    "                fmt.joinMath('=', H1*A, sp.Matrix(np.round(a1, 3))), pre=\"\\\\tiny \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "continue to zero out the lower triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = householder(a1[:, 1], 1)\n",
    "H2 = sp.MatrixSymbol('H_2', 4, 4)\n",
    "a2 = h2.dot(a1)\n",
    "fmt.displayMath(fmt.joinMath('=', H2, sp.Matrix(np.round(h2, 3))), \n",
    "                fmt.joinMath('=', H2*H1*A, sp.Matrix(np.round(a2, 3))), pre=\"\\\\tiny \")\n",
    "fmt.displayMath(\"\\;\")\n",
    "\n",
    "h3 = householder(a2[:, 2], 2)\n",
    "H3 = sp.MatrixSymbol('H_3', 4, 4)\n",
    "a3 = h3.dot(a2)\n",
    "fmt.displayMath(fmt.joinMath('=', H3, sp.Matrix(np.round(h3, 3))), \n",
    "                fmt.joinMath('=', H3*H2*H1*A, sp.Matrix(np.round(a3, 3))), pre=\"\\\\tiny \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the final results are therefore $Q = (H_3H_2H_1)^T, R = Q^T A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = (h3.dot(h2).dot(h1)).T\n",
    "r = q.T.dot(a)\n",
    "np.round(q.dot(r), 4)\n",
    "\n",
    "fmt.displayMath(fmt.joinMath('=', Q, sp.Matrix(np.round(q, 3))), fmt.joinMath('=', R, sp.Matrix(np.round(r, 3))), pre=\"\\\\tiny \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## QR decomposition for least square\n",
    "\n",
    "$$ \\min_{\\bs {\\beta}} \\Vert X \\bs{\\beta - y} \\Vert_2$$\n",
    "\n",
    "given the QR decomposition of $X = QR$:\n",
    "\n",
    "$$ \\min_{\\bs {\\beta}} \\Vert X \\bs{\\beta - y} \\Vert_2 = \\min_{\\bs {\\beta}} \\Vert Q^T X \\bs \\beta - Q^T \\bs y \\Vert_2 = \\min_{\\bs {\\beta}} \\Vert R \\bs \\beta - \\bs y'\\Vert_2$$\n",
    "\n",
    "note that $R$ is right triangular, the vector whose norm is to be minimized looks like:\n",
    "\n",
    "$$\\scriptsize \\begin{pmatrix}\n",
    "r_{11} & r_{12} & \\cdots & r_{1n} \\\\\n",
    "0 & r_{22} & \\cdots & r_{2n} \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0  & \\cdots\\ & 0 & r_{nn} \\\\\n",
    "\\hline\n",
    "0 & 0 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & 0 \n",
    "\\end{pmatrix} \n",
    "\\begin{pmatrix}\n",
    "\\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_n\n",
    "\\end{pmatrix}\n",
    "- \\begin{pmatrix}\n",
    "y'_1 \\\\ y'_2 \\\\ \\vdots \\\\ y'_n \\\\ \\hline y'_{n+1} \\\\ \\vdots \\\\ y'_m\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "therefore, the solution is the $\\bs \\beta$ that zero out the first $n$ elements of the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Top 10 numerical algorithm in the 20th century\n",
    "\n",
    "By [SIAM](https://www.siam.org/pdf/news/637.pdf):\n",
    "1. 1946, Monte Carlo\n",
    "2. 1947, Simplex method for linear programming\n",
    "3. 1950, Subspace iteration for solving $A\\bs x = \\bs y$\n",
    "4. 1951, LU decomposition\n",
    "5. 1957, Fortran optimized compiler\n",
    "6. 1959-1961, QR decomposition/QR algorithm\n",
    "7. 1963, quicksort\n",
    "8. 1965, fast fourier transform\n",
    "9. 1977, integer relation detection algorithm\n",
    "10. 1987, fast multipole algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why single out the 20th century?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Original mobile computing devices\n",
    "\n",
    "\n",
    "Slide rule:\n",
    "\n",
    "<img src=https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcS6XOdwLqDUf2PYIaP09qAMNH5RL0RAh9mA7PxPoGgKke2X6s4UAw width=400>\n",
    "\n",
    "Abacus:\n",
    "\n",
    "<img src=https://sites.google.com/site/southmath/abacus02.JPG width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "Mahatma Gandhi: action expresses priorities.\n",
    "$\\renewcommand{bt}[1]{\\tilde{\\bs #1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Principal component\n",
    "\n",
    "The principal component (PC):\n",
    "\n",
    "* Suppose $\\tilde {\\bs r}$ is a random vector, with expectation $\\bar{\\bs r} = \\mathbb{E}[\\tilde{\\bs r}]$ and covariance matrix $ V = \\mathbb{E}[(\\bt r - \\bar{\\bs r})(\\bt r - \\bar{\\bs r})^T] $\n",
    "  * e.g.: returns of a set of equity names\n",
    "\n",
    "* PC is defined to be the direction $\\hat{\\bs u}$ onto which the projection $\\tilde {\\bs r}^T \\hat{\\bs u}$ has the maximum variance, \n",
    "  * $\\hat{\\bs u}$ is a unit vector, i.e., $\\Vert \\hat{\\bs u} \\Vert_2 = \\sqrt{\\hat{\\bs u}^T\\hat{\\bs u}} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = np.random.normal(size=[3, 300])\n",
    "x = (1.5*es[0,:] + .25*es[1,:])*.3 + 1\n",
    "y = es[0,:]*.4 + es[2,:]*.2\n",
    "\n",
    "cov = np.cov([x, y])\n",
    "ev, evec = np.linalg.eig(cov)\n",
    "\n",
    "ux = mean(x)\n",
    "uy = mean(y)\n",
    "\n",
    "figure(figsize=[6, 4])\n",
    "plot(x, y, '.')\n",
    "xlim(-2, 4)\n",
    "ylim(-2, 2);\n",
    "xlabel('x')\n",
    "ylabel('y');\n",
    "\n",
    "arrow(ux, uy, -3*sqrt(ev[1])*evec[0, 1], -3*sqrt(ev[1])*evec[1, 1], width=.01, color='r')\n",
    "arrow(ux, uy, 3*sqrt(ev[0])*evec[0, 0], 3*sqrt(ev[0])*evec[1, 0], width=.01, color='r');\n",
    "title('Principal Components of 2-D Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Link to eigenvectors\n",
    "\n",
    "The projection $\\hat{\\bs u}^T \\bt r$ is a random scalar:\n",
    "\n",
    "* the variance of the projection is $\\text{var}[\\hat{\\bs u}^T \\bt r] = \\hat{\\bs u}^T V \\hat{\\bs u}$.\n",
    "* the first eigen vector $\\bs v_1$ of $V$ is therefore the principal component\n",
    "\n",
    "If we limit ourselves to all $\\bs u$ that is perpendicular to $\\bs v_1$, then: \n",
    "\n",
    "* the second eigen vector $\\bs v_2$ of $V$ is the principal component amongst all $\\bs u$ with $\\bs v_1^T \\bs u = 0$\n",
    "* this process can continue for all the eigen vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Variance explained\n",
    "\n",
    "The total variance of the random vector $\\tilde {\\bs r}$ is sum of variance of all its $n$ elements, which equals:\n",
    "\n",
    "1. trace of $V$\n",
    "2. sum of all eigen values of $V$\n",
    "\n",
    "The portion of variance explained by the first $k$ eigenvectors is $\\frac{\\sum_i^k \\lambda_i}{\\sum_i^n \\lambda_i}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Application in interest rates\n",
    "\n",
    "CMT rates are constant maturity treasury bond yield that are published daily by <a href=http://www.treasury.gov/resource-center/data-chart-center/interest-rates/Pages/TextView.aspx?data=yield>U. S. Treasury</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cmt_rates = pd.read_csv(\"data/cmt.csv\", parse_dates=[0], index_col=[0])\n",
    "\n",
    "cmt_rates.plot(legend=False, title='Historical CMT yield');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Covariance matrix\n",
    "\n",
    "* Covariance matrix between CMT rates at different maturities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tenors = cmt_rates.columns.map(float)\n",
    "cv = cmt_rates.cov()\n",
    "fmt.displayDF(cv, \"4g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## IR PCA components\n",
    "\n",
    "The eigenvectors and percentage explained, \n",
    "\n",
    "* the 1st principal component (PC) explains >95% of the variance\n",
    "* the first 3 PCs can be interpreted as level, slope and curvature\n",
    "* note that the sign of the PC is insignificant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcv, vcv = np.linalg.eig(cv)\n",
    "vcv = -vcv # flip the sign of eigen vectors for better illustratiaons\n",
    "\n",
    "pct_v = np.cumsum(xcv)/sum(xcv)*100\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.precision', 3)\n",
    "fmt.displayDF(pd.DataFrame({'P/C':range(1, len(xcv)+1), \n",
    "                            'Eigenvalues':xcv, 'Cumulative Var(%)': pct_v}).set_index(['P/C']).T, \"2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "flab = ['factor %d' % i for i in range(1, 4)]\n",
    "\n",
    "fig = figure(figsize=[12, 4])\n",
    "ax1 = fig.add_subplot(121)\n",
    "plot(tenors, vcv[:, :3], '.-');\n",
    "xlabel('Tenors(Y)')\n",
    "legend(flab, loc='best')\n",
    "title('First 3 principal components');\n",
    "\n",
    "fs = cmt_rates.dot(vcv).iloc[:, :3]\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "fs.plot(ax=ax2, title='Projections to the first 3 P/Cs')\n",
    "legend(flab, loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dimension reduction in MC\n",
    "\n",
    "Given a generic $n$-dimensional SDE in vector form:\n",
    "\n",
    "$$\\renewcommand{Sigma}{\\mathcal{S}} d \\bs x = \\bs u dt + A \\Sigma d \\bs w, \\;\\;\\; d\\bs w d\\bs w^T = C dt$$\n",
    "\n",
    "* $\\Sigma$ is a diagonal matrix of $\\sigma_i$ \n",
    "* $C$ is the correlation matrix of $d \\bs w$, its covariance matrix $V = \\Sigma C \\Sigma$.\n",
    "\n",
    "Recall Cholesky decomposition $V = LL^T$: $\\Sigma \\delta \\bs w = L \\bt z \\sqrt{\\delta t}$: \n",
    "\n",
    "$$\n",
    "\\small\n",
    "\\mathbb E [\\Sigma\\delta \\bs w (\\Sigma\\delta \\bs w)^T] = \\mathbb E[(L \\delta t \\bt z \\sqrt{\\delta t}) (L \\delta t \\bt z \\sqrt{\\delta t})^T ] \n",
    "= L \\mathbb E[\\delta t \\bt z\\delta t \\bt z^T] L^T \\delta t = V \\delta t\n",
    "$$\n",
    "\n",
    "* $\\bt z$ is independent standard normal random vector.\n",
    "* we can reduce $\\bt z$ dimension by finding a rectangular $\\dot L$ so that $V \\approx \\dot L {\\dot L}^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dimension reduction with PCA\n",
    "\n",
    "Use the PCA of the covariance matrix: \n",
    "$$\n",
    "\\scriptsize\n",
    "V = R_V \\Lambda_V R_V^T \\approx \\dot R_V \\dot \\Lambda_V \\dot R_V^T = \\dot R_V \\dot H_V \\dot H_V^T  \\dot R_V^T = (\\overbrace{\\dot R_V \\dot H_V}^{L_V})(\\dot R_V\\dot H_V )^T \\\\$$\n",
    "Or PCA on the correlation matrix:\n",
    "$$\n",
    "\\scriptsize\n",
    "V = \\Sigma C \\Sigma = \\Sigma (R_C \\Lambda_C R_C^T) \\Sigma \\approx \\Sigma \\dot R_C \\dot \\Lambda_C \\dot R_C^T \\Sigma = \\Sigma \\dot R_C \\dot H_C \\dot H_C^T  \\dot R_C^T \\Sigma^T = (\\overbrace{\\Sigma \\dot R_C \\dot H_C}^{L_C})(\\Sigma \\dot R_C\\dot H_C )^T \n",
    "$$\n",
    "\n",
    "* $\\Lambda = H H^T$, both $\\Lambda$ and $H$ are diagonal matrix with positive elements\n",
    "* The dotted version only retains the first $k$ eigen values\n",
    "* $\\dot R$ and $L_V, L_C$ are $n \\times k$ matrices; $\\dot \\Lambda$ are $k \\times k$ matrices.\n",
    "\n",
    "Simulation can then be driven by $\\Sigma \\delta \\bs w = L \\dot{\\bt z} \\sqrt{\\delta t}$:\n",
    "* Either $L_V = \\dot R_V \\dot H_V$ or $L_C = \\Sigma \\dot R_C \\dot H_C$ works\n",
    "* $\\dot{\\bt z}$ is of length $k$ only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## PCA is scale variant\n",
    "\n",
    "Given the EVDs on covariance and correlation matrices:\n",
    "\n",
    "$$R_V \\Lambda_V R_V^T = V = \\Sigma C \\Sigma = \\Sigma R_C \\Lambda_C R_C^T \\Sigma$$\n",
    "\n",
    "Are they equivalent? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\Sigma R_C \\Lambda_C R_C^T \\Sigma = (\\Sigma R_C) \\Lambda_C (\\Sigma R_C)^T$\n",
    "  * implies $\\Lambda_C = \\Lambda_V$ and $R_V = \\Sigma R_C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\Sigma R_C \\Lambda_C R_C^T \\Sigma = R_C (\\Sigma \\Lambda_C \\Sigma) R_C^T$\n",
    "  * implies $ \\Lambda_V = \\Sigma \\Lambda_C \\Sigma $ and $R_C = R_V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, neither is true because: \n",
    "* $\\Sigma R_C$ is not orthogonal: $(\\Sigma R_C)(\\Sigma R_C)^T = \\Sigma R_C R_C^T \\Sigma = \\Sigma\\Sigma \\ne I$\n",
    "  * $V = \\Sigma C \\Sigma$, $V$ and $C$ are not similar\n",
    "* $\\Sigma R_C \\ne R_C \\Sigma$, even when $\\Sigma$ is diagonal.\n",
    "\n",
    "There is no simple relationship btw these two EVDs.\n",
    "* despite $C$ and $V$ are covariance matrix of $\\bt r$ and $\\Sigma^{-1} \\bt r$ respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Orthogonality depends on scale\n",
    "\n",
    "The PCs are no longer orthogonal after stretching horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=[13, 4])\n",
    "subplot(1, 2, 1)\n",
    "\n",
    "plot(x, y, '.')\n",
    "xlim(-2, 4)\n",
    "ylim(-2, 2);\n",
    "xlabel('x')\n",
    "ylabel('y');\n",
    "\n",
    "arrow(ux, uy, -3*sqrt(ev[1])*evec[0, 1], -3*sqrt(ev[1])*evec[1, 1], width=.01, color='r')\n",
    "arrow(ux, uy, 3*sqrt(ev[0])*evec[0, 0], 3*sqrt(ev[0])*evec[1, 0], width=.01, color='r');\n",
    "title('Unscaled PCs');\n",
    "\n",
    "subplot(1, 2, 2)\n",
    "\n",
    "scale = 1.5\n",
    "plot(x*scale, y, '.')\n",
    "xlim(-2, 4)\n",
    "ylim(-2, 2);\n",
    "xlabel('x')\n",
    "ylabel('y');\n",
    "\n",
    "arrow(ux*scale, uy, -3*sqrt(ev[1])*evec[0, 1]*scale, -3*sqrt(ev[1])*evec[1, 1], width=.01, color='r')\n",
    "arrow(ux*scale, uy, 3*sqrt(ev[0])*evec[0, 0]*scale, 3*sqrt(ev[0])*evec[1, 0], width=.01, color='r');\n",
    "title('Scaled');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Before applying PCA \n",
    "\n",
    "PCA is a powerful technique, however, beware of its limitations:\n",
    "\n",
    "* PCA factors are not real market risk factors\n",
    " * It does not lead to tradable hedging strategies\n",
    "* PCA is not scale (or unit) invariant\n",
    " * it requires all the dimensions to have the same natural unit\n",
    " * mixing factors of different magnitude can lead to trouble: eg, mixing interest rates with equities\n",
    " * PCA results are different when applied to co-variance matrix and correlation matrix\n",
    " \n",
    "PCA is usually applied to different data points of the same type\n",
    "\n",
    "* different tenors/strikes of interest rates, volatility etc.\n",
    "* price movements of the same sector/asset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## PCA vs. least square\n",
    "\n",
    "The PCA analysis has some similarity to the least square problem, both of them can be viewed as minimizing the L2 norm of residual errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "xs = np.array([x, np.ones(len(x))]).T\n",
    "beta = np.linalg.inv(xs.T.dot(xs)).dot(xs.T).dot(y)\n",
    "\n",
    "s_x, s_y = sp.symbols('x, y')\n",
    "br = np.round(beta, 4)\n",
    "\n",
    "figure(figsize=[13, 4])\n",
    "subplot(1, 2, 1)\n",
    "plot(x, y, '.')\n",
    "xlim(-2, 4)\n",
    "ylim(-2, 2);\n",
    "xlabel('x')\n",
    "ylabel('y');\n",
    "\n",
    "dx = 4*sqrt(ev[0])*evec[0, 0]\n",
    "dy = 4*sqrt(ev[0])*evec[1, 0]\n",
    "\n",
    "arrow(ux-dx, uy-dy, 2*dx, 2*dy, width=.01, color='k');\n",
    "\n",
    "ex = 3*sqrt(ev[1])*evec[0, 1]\n",
    "ey = 3*sqrt(ev[1])*evec[1, 1]\n",
    "plot([ux, ux+ex], [uy, uy+ey], 'r-o', lw=3)\n",
    "legend(['data', 'residual error'], loc='best')\n",
    "\n",
    "title('PCA');\n",
    "\n",
    "subplot(1, 2, 2)\n",
    "plot(x, y, '.')\n",
    "xlim(-2, 4)\n",
    "ylim(-2, 2);\n",
    "xlabel('x')\n",
    "ylabel('y');\n",
    "arrow(-1, -1*beta[0]+beta[1], 4, 4*beta[0], width=.01, color='k');\n",
    "y0 = (ux+ex)*beta[0] + beta[1]\n",
    "plot([ux+ex, ux+ex], [y0, uy+ey], 'r-o', lw=3)\n",
    "legend(['data', 'residual error'], loc='best')\n",
    "title('Least Square');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## PCA or regression?\n",
    "\n",
    "The main differences between PCA and least square (regression):\n",
    "\n",
    "|  | PCA | Least Square/Regression |\n",
    "| :-----: | :----: | :----: |\n",
    "| Scale Invariant | No | Yes |\n",
    "| Symmetry in Dimension | Yes | No |\n",
    "\n",
    "To choose between the two:\n",
    "* Use least square/regression when there are clear explanatory variables and causality\n",
    "* Use PCA when there is a set of related variables but no clear causality\n",
    "* PCA requires a natural unit for all dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What variable to apply PCA\n",
    "\n",
    "It is important to choose the right variable to apply PCA. The main considerations are:\n",
    "\n",
    "* Values or changes\n",
    " * Equity prices or returns?\n",
    " * IR levels or changes?\n",
    " * Key consideration: stationarity and mean reversion, horizon of your model prediction\n",
    " \n",
    "* Spot or forward\n",
    " * Forward is usually a better choice than spot\n",
    " \n",
    "Since the PCA is commonly applied to a correlation/covariance matrix, it is equivalent to ask what variables' correlation/covariance matrix we should model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spurious correlation - equity price\n",
    "\n",
    "Spurious correlation: variables can appear to be significantly correlated, but it is just an illusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = pd.read_csv('data/f3.csv', parse_dates=[0]).set_index('Date').sort_index()\n",
    "r = np.log(f3).diff()\n",
    "\n",
    "fmt.displayDFs(f3.corr(), r.corr(), headers=['Price Correlation', 'Return Correlation'], fmt=\"4g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spurious correlation - spot rates\n",
    "\n",
    "The correlation between 9Y and 10Y spot rates and forward rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "es = np.random.normal(size=[2, 500])\n",
    "r0 = es[0,:]*.015 + .05\n",
    "f1 = es[1,:]*.015 + .05\n",
    "\n",
    "r1 = -np.log(np.exp(-9*r0-f1))/10\n",
    "\n",
    "\n",
    "rc = np.corrcoef(np.array([r0, r1]))[0, 1]\n",
    "fc = np.corrcoef(np.array([r0, f1]))[0, 1]\n",
    "\n",
    "figure(figsize=[12, 4])\n",
    "subplot(1, 2, 1)\n",
    "plot(r0, r1, '.')\n",
    "xlabel('9Y rate')\n",
    "ylabel('10Y spot')\n",
    "title('Corr(9Y Spot, 10Y Spot) = %.4f'% rc)\n",
    "\n",
    "subplot(1, 2, 2)\n",
    "plot(r0, f1, '.')\n",
    "xlabel('9Y rate')\n",
    "ylabel('9-10Y forward')\n",
    "title('Corr(9Y Spot, 9-10Y Forward) = %.4f'% fc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{eqnarray}\n",
    "\\exp(-10 r_{10}) &=& \\exp(-9 r_9)\\exp(-(10-9) f(9, 10) )  \\\\\n",
    "r_{10} &=& 0.9r_9 + 0.1f(9, 10) \n",
    "\\end{eqnarray}$$\n",
    "\n",
    "* the $f(9, 10)$ is the 9Y to 10Y forward rate.\n",
    "* of course $r_{10} = f(0, 10)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Correlated by construction\n",
    "\n",
    "$$ r_{10} = 0.9r_9 + 0.1f(9, 10) $$\n",
    "\n",
    "* The high correlation between spot rates to similar tenors are by construction\n",
    "* PCA analysis can be misleading in percentage of variance explained\n",
    "  * the variance of spot rates of short tenors are counted multiple times\n",
    "* May not be a problem if we have unlimited precision, but we usually only keep the first few eigen vectors in PCA\n",
    "* it's usually better to model the correlation/covariance matrix of forward rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Singular Value Decomposition\n",
    "\n",
    "rotate, stretch and rotate, then you get back to square one, that's pretty much the life in a bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Definition\n",
    "\n",
    "For any real matrix $M$, $\\sigma > 0$ is a singular value if there exists two vectors $\\bs {u, v}$ such that:\n",
    "\n",
    "1. $M \\bs v = \\sigma \\bs u$ \n",
    "2. $M^T \\bs u = \\sigma \\bs v$\n",
    "\n",
    "$\\bs {u, v}$ are called left and right singular vector of $M$.\n",
    "* unlike eigenvalues, singular values are always positive.\n",
    "\n",
    "By convention, \n",
    "* we represent singular vectors as unit L-2 vectors, i.e., $\\bs u^T\\bs u = \\bs v^T \\bs v = 1$,\n",
    "* we name the singular values in descending order as $\\sigma_1, \\sigma_2, ..., \\sigma_n$, and refer corresponding singular vector pairs as $ (\\bs u_1, \\bs v_1), (\\bs u_2, \\bs v_2), ..., (\\bs u_n, \\bs v_n) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Singular vectors and maximization\n",
    "\n",
    "Consider $\\bs u^T M \\bs v$ for real matrix $M$:\n",
    "\n",
    "* $\\bs {u_1, v_1}$ maximize $\\bs u^T M \\bs v$ amongst all unit $\\bs {u, v}$ vectors. \n",
    "\n",
    "Apply the Lagrange multiplier, with constraints $\\bs u^T  \\bs u = \\bs v^T  \\bs v = 1$:\n",
    "\n",
    "$$\\small \\begin{array} \n",
    "\\\\l &= \\bs u^T M \\bs v - \\lambda_1 (\\bs u^T  \\bs u - 1) - \\lambda_2 (\\bs v^T  \\bs v - 1) \\\\\n",
    "\\frac{\\partial l}{\\partial{\\bs u^T}} &= M \\bs v - 2 \\lambda_1 \\bs u = \\bs 0 \\iff \\bs u^T M \\bs v - 2 \\lambda_1 = \\bs 0\\\\\n",
    "\\frac{\\partial l}{\\partial{\\bs v}} &= \\bs u^T M  - 2\\lambda_2 \\bs v^T = \\bs 0^T \\iff \\bs u^T M \\bs v- 2\\lambda_2 = \\bs 0\n",
    "\\end{array}$$\n",
    "\n",
    "Therefore,\n",
    "$$ 2\\lambda_1 = \\small \\bs u^T M \\bs v  = 2\\lambda_2 $$\n",
    "$$M \\bs v = \\sigma \\bs u \\;,\\; M^T\\bs u = \\sigma \\bs v $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Orthogonality of Singular Vectors\n",
    "\n",
    "Given $\\bs{u_1, v_1}$ are the first singular vectors of $M$:\n",
    "\n",
    "$$\\bs x^T \\bs v_1 = 0  \\iff (M \\bs x)^T \\bs u_1 = 0 \\iff \\bs u_1^T M \\bs x  = 0 $$\n",
    "\n",
    "because:\n",
    "\n",
    "$$ (M \\bs x)^T \\bs u_1 = \\bs x^T M^T \\bs u_1 = \\bs x^T \\sigma_1 \\bs v_1  = 0 $$\n",
    "\n",
    "Therefore, if we only consider those $\\bs u^T \\bs u_1^ = 0$, the corresponding $\\bs v$ must be orthogonal to $\\bs v_1$ :\n",
    "\n",
    "* $\\bs {u_2, v_2}$ maximizes $\\bs u^T M \\bs v$  among those $(\\bs u, \\bs v)$ orthogonal to $\\bs u_1, \\bs v_1$\n",
    "* this process can repeat for all singular vectors (assuming singular values are distinct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Singular value decomposition\n",
    "\n",
    "We can write all singular vectors and singular values in matrix format, which is the singular value decomposition (SVD):\n",
    "\n",
    "$$\n",
    "U^T M V = \\Sigma \\iff M = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "| | $M$ | $U$ | $\\Sigma$ | $V$ |\n",
    "| :---: | :---: | :----: | :----: | :---: |\n",
    "| Name | Original Matrix | Left singular vector | Singular value | Right singular vector |\n",
    "| Type |  Real | Orthogonal, real | Diagonal, positive | Orthogonal, real |\n",
    "| Size | $m \\times n$ | $m\\times m$ | $m\\times n$ | $n \\times n$ |\n",
    "\n",
    "* For matrix $M$, there can only be $\\min(m, n)$ singular values at most\n",
    "* The left/right singular vectors form a basis for the vector space of $\\bs {u}$ and $\\bs v$ respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Graphical representation of SVD\n",
    "\n",
    "Illustration from wikipedia:\n",
    "\n",
    "<center> <img src=\"img/svd.png\" width=500 height=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Link between SVD and EVD\n",
    "\n",
    "\n",
    "$$\n",
    "M = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "SVD and eigenvalue decomposition (EVD) are closely related to each other:\n",
    "\n",
    "* SVD reduces to EVD if $M$ is symmetric positive definite\n",
    "* $MM^T = U \\Sigma V^T V \\Sigma^T U^T = U \\Sigma \\Sigma^T U^T$:\n",
    "  * the column vectors of $U$ are the eigen vectors of $MM^T$\n",
    "* $M^TM = V\\Sigma^TU^TU\\Sigma V^T = V \\Sigma\\Sigma^T V^T$: \n",
    "  * the column vectors of $V$ are the eigen vectors of $M^TM$\n",
    "* The singular values are the positive square roots of eigen values of $MM^T$ or $M^TM$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SVD of pseudo inverse\n",
    "\n",
    "Given the SVD decomposition $X = U\\Sigma V^T$, the SVD of $X$'s pseudo inverse is:\n",
    "\n",
    "$$X^+ = (X^TX)^{-1}X^T = V \\Sigma^+ U^T$$\n",
    "\n",
    "where $\\Sigma^+$ is the pseudo inverse of $\\Sigma$, whose diagonal elements are the reciprocals of those in $\\Sigma$.\n",
    "\n",
    "Proof: the last equation is obviously true, and every step is reversible:\n",
    "\n",
    "$$\\begin{array}&(X^TX)^{-1}X^T &= V\\Sigma^+ U^T \\\\\n",
    "(V \\Sigma^T \\Sigma V^T)^{-1} V\\Sigma^TU^T &= V\\Sigma^+ U^T \\\\\n",
    "(V \\Sigma^T \\Sigma V^T)^{-1} V\\Sigma^T\\Sigma &= V \\Sigma^+\\Sigma \\\\\n",
    "(V \\Sigma^T \\Sigma V^T)^{-1} V\\Sigma^T\\Sigma &= V \\\\\n",
    "(V \\Sigma^T \\Sigma V^T)^{-1} (V\\Sigma^T\\Sigma V^T) &= I \n",
    "\\end{array}$$\n",
    "\n",
    "SVD is another way to solve the least square problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## L2 norm and condition number\n",
    "\n",
    "Consider a real matrix $A$, its L2 norm is defined as the maximum of $\\frac{\\Vert A \\bs x \\Vert_2}{\\Vert \\bs x \\Vert_2}$:\n",
    "\n",
    "$$\n",
    "\\Vert A \\Vert_2^2 = \\max_{\\bs x} \\frac{\\Vert A \\bs x \\Vert_2^2}{\\Vert \\bs x \\Vert_2^2} \n",
    "= \\max_{\\bs x} \\frac{\\bs x^T A^T A \\bs x}{\\bs x^T \\bs x} \n",
    "= \\max_{\\hat{\\bs x}} \\hat{\\bs x}^T A^T A \\hat{\\bs x} = \\sigma_1(A)^2$$\n",
    "\n",
    "where $\\hat{\\bs x} = \\frac{\\bs x}{\\Vert \\bs x \\Vert_2}$ is a unit vector, and $\\sigma_1(A)$ is the largest singular value of $A$.\n",
    "\n",
    "For a generic non-square matrix $A$:\n",
    "* the L-2 norm of a matrix is its largest singular value: $\\Vert A \\Vert_2 = \\sigma_1(A)$\n",
    "* the L-2 norm of the pseudo inverse $A^+$: $\\Vert A^+ \\Vert_2 = \\frac{1}{\\sigma_n(A)}$\n",
    "  * $\\sigma_n(A)$ is the smallest singular value of $A$.\n",
    "* the L-2 condition number of a matrix $A$ is therefore: $k(A) = \\Vert A \\Vert_2 \\Vert A^+ \\Vert_2 = \\frac{\\sigma_1(A)}{\\sigma_n(A)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SVD and condition number\n",
    "\n",
    "SVD offers clear intuitions in understanding the L-2 condition number of a Matrix $A$.\n",
    "\n",
    "For a linear system $\\bs y = A \\bs x$, consider a perturbation $\\delta \\bs x$ to the input $\\bs x$:\n",
    "\n",
    "* if $\\delta \\bs x$ is stretched by a larger factor than $\\bs x$, then the relative error grows.\n",
    "* the worst case occurs when $\\bs x$ is stretched by $\\sigma_n(A)$, while $\\delta \\bs x$ is stretched by $\\sigma_1(A)$\n",
    "  * the relative error grows by a factor of $\\frac{\\sigma_1(A)}{\\sigma_n(A)}$.\n",
    "* Orthogonal matrix represents a pure rotation\n",
    "  * all singular values are 1, no stretches, thus the relative error does not grow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spot the trouble\n",
    "\n",
    "When working with near singular matrix,  SVD can identify ill-conditioned area:\n",
    "\n",
    "* real world covariance and correlation matrix are often near singular\n",
    "* numerical instability arises when inputs are very close to signular vectors with small singular values \n",
    "  * e.g, when return forecast is in the same direction as the last singular vector ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Rank deficiency and reduction\n",
    "\n",
    "Singular matrix are those that are not fully ranked\n",
    "* Square matrix: non-invertible, its determinant is 0\n",
    "* In practice, matrices are rarely exactly singular due to numerical errors\n",
    "\n",
    "SVD can detect near singular matrices\n",
    "* near singular matrix has very small singular values\n",
    "\n",
    "SVD can find low rank approximations to a full rank matrix\n",
    "* by zeroing out small singular values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Condition number for EVD\n",
    "\n",
    "What is the change in $\\lambda_i$ if we perturb the symmetric matrix $A$? \n",
    "\n",
    "$$ A \\bs v_i = \\lambda_i \\bs v_i$$\n",
    "\n",
    "Apply the perturbation analysis: $\\delta A = \\dot{A} \\epsilon, \\delta \\lambda_i = \\dot{\\lambda_i} \\epsilon, \n",
    "\\delta \\bs v_i = \\dot{\\bs v_i} \\epsilon$, the first order terms of $\\epsilon$:\n",
    "\n",
    "$$\\begin{array} \n",
    "\\\\ (A + \\dot{A}\\epsilon )(\\bs v_i + \\dot{\\bs v_i}\\epsilon) &= (\\lambda_i + \\dot{\\lambda_i}\\epsilon) (\\bs v_i + \\dot{\\bs v_i} \\epsilon) \\\\\n",
    "\\dot{A}\\bs v_i + A \\dot{\\bs v_i} &= \\lambda_i \\dot{\\bs v_i} + \\dot{\\lambda_i}\\bs v_i \\\\\n",
    "\\bs v_i^T\\dot{A}\\bs v_i + \\bs v_i^T A \\dot{\\bs v_i} &= \\bs v_i^T \\lambda_i \\dot{\\bs v_i} + \\bs v_i^T \\dot{\\lambda_i}\\bs v_i \\\\\n",
    "\\bs v_i^T\\dot{A}\\bs v_i &= \\dot{\\lambda_i}\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Therefore:\n",
    "$$\\begin{array}\n",
    "\\\\ \\frac{ | \\delta \\lambda_i |}{| \\lambda_i |} &= \\frac{\\Vert \\bs v_i^T\\delta A \\bs v_i\\Vert_2}{|\\lambda_i|}\n",
    "\\le \\frac{\\Vert \\delta A \\Vert_2}{|\\lambda_i|} = \\frac{\\Vert A\\Vert_2}{|\\lambda_i|} \\frac{\\Vert \\delta A \\Vert_2}{\\Vert A \\Vert_2}\n",
    " = \\left\\vert \\frac{\\lambda_1}{\\lambda_i}\\right\\vert \\frac{\\Vert \\delta A \\Vert_2}{\\Vert A \\Vert_2} \n",
    " \\end{array}$$\n",
    " \n",
    "* The eigenvalue's conditional number for a symmetric matrix $A$ is therefore the ratio from the largest eigen value: $k(A) = \\left\\vert \\frac{\\lambda_1}{\\lambda_i} \\right\\vert$.\n",
    "* The smaller eigenvalues are less accurate, we should ignore them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decomposition summary\n",
    " \n",
    "For real matrix $A$:\n",
    " \n",
    "| Decomposition | Applicability | Formula | Results | Key applications |\n",
    "| :---: | :---: | :---: | :---: | \n",
    "| LU | square, full rank | $A = LU$ | lower, upper triangular | matrix inversion |\n",
    "| Cholesky | symmetric positive definite | $A = LL^T$ | lower triangular | simulate correlated factors |\n",
    "| QR | any | $A = Q R$ | orthogonal, upper triangular | solve eigen values, least square|\n",
    "| EVD | symmetric | $A = R\\Lambda R^T$ | orthogonal, real diagonal | PCA |\n",
    "| SVD | any | $A = U\\Sigma V^T$ | orthogonal, positive diagonal, orthogonal | error analysis, rank reduction |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment\n",
    "\n",
    "Required reading:\n",
    "\n",
    "* Bindel and Goodman: Chapter 4\n",
    "\n",
    "Recommended reading:\n",
    "\n",
    "* Andersen and Piterbarg: 4.1\n",
    "\n",
    "Homework: \n",
    "\n",
    "* [Homework set 3](http://yadongli.github.io/nyumath2048/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
